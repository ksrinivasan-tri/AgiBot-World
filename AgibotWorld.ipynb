{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgiBot World Diffusion Policy Training Demo\n",
    "\n",
    "This notebook demonstrates how to use **AgiBotWorldDataset** to run an offline training workflow.\n",
    "Make sure you have installed all necessary packages before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/datasets/utils/experimental.py:37: UserWarning: 'register_feature' is experimental and might be subject to breaking changes in the future.\n",
      "  warnings.warn(\n",
      "Overwriting feature type 'Image' (Image -> Image)\n"
     ]
    }
   ],
   "source": [
    "from scripts.image import Image as ImageFeature\n",
    "from datasets.features.features import register_feature\n",
    "from datasets.features import Image as DeprecatedImageFeature\n",
    "\n",
    "# replace image feature with new one\n",
    "register_feature(ImageFeature, DeprecatedImageFeature.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ef21ac7f2a42f9a8bbe971e87373d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "TASK_ID = 362\n",
    "repo_id = f\"agibotworld/task_{TASK_ID}\"\n",
    "tgt_path = \"/home/ubuntu/lerobot_data/\"\n",
    "dataset_root = f\"{tgt_path}/{repo_id}\"\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_dir=Path(dataset_root) / \"data\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation.images.cam_top_depth': <PIL.TiffImagePlugin.TiffImageFile image mode=F size=640x480 at 0x7883CAF8C3D0>,\n",
       " 'observation.state': [-1.7821335792541504,\n",
       "  0.4922644793987274,\n",
       "  2.12530517578125,\n",
       "  -1.1757111549377441,\n",
       "  2.940516948699951,\n",
       "  -0.885674774646759,\n",
       "  -1.6433712244033813,\n",
       "  1.3742921352386475,\n",
       "  -0.8007281422615051,\n",
       "  -1.2186731100082397,\n",
       "  0.3783508837223053,\n",
       "  -0.48504018783569336,\n",
       "  -1.4048820734024048,\n",
       "  -1.4678765535354614,\n",
       "  34.93333435058594,\n",
       "  35.022220611572266,\n",
       "  0.0,\n",
       "  0.5060499906539917,\n",
       "  0.663100004196167,\n",
       "  0.2749898433685303],\n",
       " 'action': [-1.7821335792541504,\n",
       "  0.4922644793987274,\n",
       "  2.12530517578125,\n",
       "  -1.1757111549377441,\n",
       "  2.940516948699951,\n",
       "  -0.885674774646759,\n",
       "  -1.6433712244033813,\n",
       "  1.3742921352386475,\n",
       "  -0.8007281422615051,\n",
       "  -1.2186731100082397,\n",
       "  0.3783508837223053,\n",
       "  -0.48504018783569336,\n",
       "  -1.4048820734024048,\n",
       "  -1.4678765535354614,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5060499906539917,\n",
       "  0.663100004196167,\n",
       "  0.2749898433685303,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'episode_index': 0,\n",
       " 'frame_index': 0,\n",
       " 'index': 0,\n",
       " 'task_index': 0,\n",
       " 'timestamp': 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation.images.cam_top_depth': Image(mode=None, decode=True, id=None),\n",
       " 'observation.state': Sequence(feature=Value(dtype='float32', id=None), length=20, id=None),\n",
       " 'action': Sequence(feature=Value(dtype='float32', id=None), length=22, id=None),\n",
       " 'episode_index': Value(dtype='int64', id=None),\n",
       " 'frame_index': Value(dtype='int64', id=None),\n",
       " 'index': Value(dtype='int64', id=None),\n",
       " 'task_index': Value(dtype='int64', id=None),\n",
       " 'timestamp': Value(dtype='float32', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['observation.images.cam_top_depth', 'observation.state', 'action', 'episode_index', 'frame_index', 'index', 'task_index', 'timestamp'],\n",
       "    num_shards: 714\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Returning existing local_dir `/home/ubuntu/lerobot_data/agibotworld/task_362` as remote repo cannot be accessed in `snapshot_download` (None).\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses of Dataset should implement __getitem__.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_285793/4185017502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeRobotDatasetMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mepisode_data_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_episode_data_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcheck_timestamps_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_data_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/lerobot/common/datasets/utils.py\u001b[0m in \u001b[0;36mcheck_timestamps_sync\u001b[0;34m(hf_dataset, episode_data_index, fps, tolerance_s, raise_value_error)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0maccount\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpossible\u001b[0m \u001b[0mnumerical\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0mtimestamps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mwithin_tolerance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T_co\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Subclasses of Dataset should implement __getitem__.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# def __getitems__(self, indices: List) -> List[_T_co]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Subclasses of Dataset should implement __getitem__."
     ]
    }
   ],
   "source": [
    "\n",
    "from lerobot.common.datasets.utils import check_timestamps_sync\n",
    "from lerobot.common.datasets.utils import get_episode_data_index\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDatasetMetadata\n",
    "\n",
    "meta = LeRobotDatasetMetadata(repo_id, dataset_root, True)\n",
    "episode_data_index = get_episode_data_index(meta.episodes, None)\n",
    "check_timestamps_sync(dataset, episode_data_index, 30, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_277414/4201048562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Check if the dataset directory exists and has parquet files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mparquet_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from scripts.convert_to_lerobot import AgiBotDataset\n",
    "\n",
    "# Check if the dataset directory exists and has parquet files\n",
    "parquet_files = list(Path(dataset_root).rglob(\"*.parquet\"))\n",
    "resume = 0\n",
    "\n",
    "processed_episodes = len(parquet_files)\n",
    "print(f\"Found {processed_episodes} processed episodes. Resuming from episode {processed_episodes}.\")\n",
    "resume = processed_episodes\n",
    "\n",
    "# Create or load the dataset\n",
    "dataset = AgiBotDataset(\n",
    "    repo_id=repo_id,\n",
    "    root=dataset_root,  # Use dataset_root instead of tgt_path\n",
    "    local_files_only=True,\n",
    "    download_videos=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Returning existing local_dir `/home/ubuntu/lerobot_data/agibotworld/task_327` as remote repo cannot be accessed in `snapshot_download` (None).\n",
      "Returning existing local_dir `/home/ubuntu/lerobot_data/agibotworld/task_327` as remote repo cannot be accessed in `snapshot_download` (None).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bef0467f8274a50b4581af2f9be793e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9ea65d6994491f90407051451cd4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with modified dataset: 'NoneType' object has no attribute 'seek'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_7679/2651213137.py\", line 22, in <module>\n",
      "    first_item = modified_dataset[0]\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lerobot/common/datasets/lerobot_dataset.py\", line 645, in __getitem__\n",
      "    item = self.hf_dataset[idx]\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2861, in __getitem__\n",
      "    return self._getitem(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2846, in _getitem\n",
      "    formatted_output = format_table(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 633, in format_table\n",
      "    return formatter(pa_table, query_type=query_type)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 397, in __call__\n",
      "    return self.format_row(pa_table)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 486, in format_row\n",
      "    formatted_batch = self.format_batch(pa_table)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 515, in format_batch\n",
      "    batch = self.python_features_decoder.decode_batch(batch)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 222, in decode_batch\n",
      "    return self.features.decode_batch(batch) if self.features else batch\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/features/features.py\", line 2018, in decode_batch\n",
      "    [\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/features/features.py\", line 2019, in <listcomp>\n",
      "    decode_nested_example(self[column_name], value, token_per_repo_id=token_per_repo_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/features/features.py\", line 1341, in decode_nested_example\n",
      "    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/features/image.py\", line 189, in decode_example\n",
      "    if image.getexif().get(PIL.Image.ExifTags.Base.Orientation) is not None:\n",
      "  File \"/usr/lib/python3/dist-packages/PIL/Image.py\", line 1360, in getexif\n",
      "    self._exif.load_from_fp(self.fp, self.tag_v2._offset)\n",
      "  File \"/usr/lib/python3/dist-packages/PIL/Image.py\", line 3410, in load_from_fp\n",
      "    self.fp.seek(offset)\n",
      "AttributeError: 'NoneType' object has no attribute 'seek'\n"
     ]
    }
   ],
   "source": [
    "# Try a more controlled approach to access the dataset\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "# Create a modified dataset that only loads specific features and frames\n",
    "try:\n",
    "    # Define delta timestamps that only load current frames (no history or future)\n",
    "    delta_timestamps = {\n",
    "        \"observation.state\": [0],\n",
    "        \"action\": [0]\n",
    "    }\n",
    "    \n",
    "    # Create the dataset with limited features\n",
    "    modified_dataset = LeRobotDataset(\n",
    "        repo_id=repo_id,\n",
    "        root=dataset_root,\n",
    "        delta_timestamps=delta_timestamps,  # Only load current frames\n",
    "        download_videos=False,  # Don't try to redownload videos\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    # Try to access the first item\n",
    "    first_item = modified_dataset[0]\n",
    "    print(\"Successfully accessed first item\")\n",
    "    print(f\"Available keys: {list(first_item.keys())}\")\n",
    "    \n",
    "    # Now try to load one video frame at a time\n",
    "    for video_key in modified_dataset.meta.video_keys[:1]:  # Start with just the first video\n",
    "        try:\n",
    "            # Use the built-in methods to get a specific video frame\n",
    "            frame = modified_dataset.get_frames(0, video_key, [0])[0]  # Get frame at timestamp 0\n",
    "            print(f\"Successfully loaded frame from {video_key}, shape: {frame.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {video_key}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with modified dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 1. Imports and Parameter Settings\n",
    "# =============================================\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "\n",
    "# Parameters\n",
    "FPS = 30\n",
    "TASK_ID = 362\n",
    "training_steps = 5000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"/home/ubuntu/lerobot_data/\"\n",
    "# output_path = \"/path/to/save/your/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Returning existing local_dir `/home/ubuntu/lerobot_data/agibotworld/task_362` as remote repo cannot be accessed in `snapshot_download` (None).\n",
      "Returning existing local_dir `/home/ubuntu/lerobot_data/agibotworld/task_362` as remote repo cannot be accessed in `snapshot_download` (None).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6edafd73ef43fa8f5c90507d4023d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca75134b626476b8824492df65ec79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/714 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc162c26a9645e88761342343a15f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================\n",
    "# 2. Dataset Setup\n",
    "# =============================================\n",
    "observation_idx = np.array([-1, 0])\n",
    "action_idx = np.arange(-1, 15)\n",
    "repo_id = f\"agibotworld/task_{TASK_ID}\"\n",
    "\n",
    "delta_timestamps = {\n",
    "    \"observation.images.top_head\": (observation_idx / FPS).tolist(),\n",
    "    \"observation.state\": (observation_idx / FPS).tolist(),\n",
    "    \"action\": (action_idx / FPS).tolist(),\n",
    "}\n",
    "\n",
    "dataset = LeRobotDataset(\n",
    "    repo_id=repo_id,\n",
    "    root=f\"{dataset_path}/{repo_id}\",\n",
    "    delta_timestamps=delta_timestamps,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=0,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train one robot policy model to master multiple distinct skills, you can use ’MultiLeRobotDataset‘ to load datasets for various tasks into a unified training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from lerobot.common.datasets.lerobot_dataset import MultiLeRobotDataset\n",
    "repo_ids = [f\"agibotworld/{path.name}\" for path in Path(dataset_path).glob(\"agibotworld/task_*\")]\n",
    "multi_dataset = MultiLeRobotDataset(\n",
    "    repo_ids=repo_ids,\n",
    "    root=dataset_path,\n",
    "    delta_timestamps=delta_timestamps,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's kick off a simple training with Diffusion Policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 3. Policy Configuration and Initialization\n",
    "# =============================================\n",
    "cfg = DiffusionConfig()\n",
    "cfg.input_shapes = {\n",
    "    \"observation.images.top_head\": [3, 480, 640],\n",
    "    \"observation.state\": [20],\n",
    "}\n",
    "cfg.input_normalization_modes = {\n",
    "    \"observation.images.top_head\": \"mean_std\",\n",
    "    \"observation.state\": \"min_max\",\n",
    "}\n",
    "cfg.output_shapes = {\n",
    "    \"action\": [22],\n",
    "}\n",
    "\n",
    "policy = DiffusionPolicy(cfg, dataset_stats=dataset.meta.stats)\n",
    "#policy = DiffusionPolicy(cfg, dataset_stats=multi_dataset.stats)\n",
    "policy.train()\n",
    "policy.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 4. Training Loop\n",
    "# =============================================\n",
    "step = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "        output_dict = policy.forward(batch)\n",
    "        loss = output_dict[\"loss\"]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"Step {step}, Loss: {loss.item():.3f}\")\n",
    "        step += 1\n",
    "        \n",
    "        if step >= training_steps:\n",
    "            done = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 5. Save Policy Checkpoint\n",
    "# =============================================\n",
    "policy.save_pretrained(output_path)\n",
    "print(f\"Model saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! Now please feel free to explore the AgiBot World!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
